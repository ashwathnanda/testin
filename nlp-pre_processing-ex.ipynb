{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.0.17)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample HTML file to be pre-processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "[Some text we don't want to keep is in here]\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "[This is some other unwanted text]\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "&nbsp;&nbsp;\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "22    45   1067   445\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "{Here is more stuff in single curly braces.}\n",
    "[DELETE]\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "  \n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "22    45   1067   445\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "sample = denoise_text(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "  \n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "22    45   1067   445\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "sample = replace_contractions(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', 'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', 'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', 'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', 'curly', 'braces']\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(words):\n",
    "    #Remove non-ASCII characters from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = normalize(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:\n",
      " ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', 'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', 'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', 'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', 'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', 'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\n",
      "\n",
      "Lemmatized:\n",
      " ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', 'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\n"
     ]
    }
   ],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "stems, lemmas = stem_and_lemmatize(words)\n",
    "print('Stemmed:\\n', stems)\n",
    "print('\\nLemmatized:\\n', lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Rake library to extract keywords from a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rake-nltk) (3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk->rake-nltk) (1.11.0)\n",
      "Building wheels for collected packages: rake-nltk\n",
      "  Running setup.py bdist_wheel for rake-nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
      "Successfully built rake-nltk\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text('code to retrive model data from the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retrive model data', 'database', 'code']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9.0, 'retrive model data'), (1.0, 'database'), (1.0, 'code')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['find', 'fibonacci', 'series']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('find fibonacci series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FLASHTEXT to extract keywords from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flashtext\n",
      "  Downloading https://files.pythonhosted.org/packages/81/d8/2cd0656eae456d615c2f1efbcae8dfca2cb871a31f34ba8925aba47d5e09/flashtext-2.7.tar.gz\n",
      "Building wheels for collected packages: flashtext\n",
      "  Running setup.py bdist_wheel for flashtext ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/db/d7/fe74f7cb8e5c3afed90fe6f4967c933a6f13d81ab6b3d3128c\n",
      "Successfully built flashtext\n",
      "Installing collected packages: flashtext\n",
      "Successfully installed flashtext-2.7\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_processor = KeywordProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_processor.add_keyword('fibonacci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = key_processor.extract_keywords('a code to find fibonacci series of a number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fibonacci']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_processor.add_keyword('database' , 'relational database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relational database', 28, 36)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_processor.extract_keywords('code to extract data from a database' , span_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_processor.add_keywords_from_list(['python','java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'fibonacci']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_processor.extract_keywords('Python code to find fibonacci series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
