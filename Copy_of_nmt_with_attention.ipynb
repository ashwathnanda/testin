{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nmt_with_attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwathnanda/testin/blob/master/Copy_of_nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural Machine Translation with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "outputId": "06248eac-c2a1-40bc-f9a1-773756617bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 58kB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Installing collected packages: tf-estimator-nightly, google-pasta, tb-nightly, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "colab": {}
      },
      "source": [
        "# Download the file\n",
        "\n",
        "path_to_file = \"keyword-data.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opI2GzOt479E",
        "outputId": "35608b40-1d2c-49ad-c123-da04721f4c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTbSbBz55QtF",
        "outputId": "34cb7996-7ffe-413a-ded3-77a626c3b349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> gmail , google calendar , docs , and talk leave beta <end>\n",
            "<start> google gmail google calendar beta <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmMZQpdO60dt",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    inp_lang, targ_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "c7ee5596-4bcd-48d2-8b17-b1412e2944a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1432, 1432, 359, 359)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "outputId": "2e6803c8-6cd7-4ccd-d355-39d015b3e586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "6 ----> best\n",
            "2535 ----> bank\n",
            "11 ----> for\n",
            "473 ----> high\n",
            "1241 ----> interest\n",
            "1242 ----> savings\n",
            "823 ----> accounts\n",
            "14 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "242 ----> savings\n",
            "424 ----> interest\n",
            "242 ----> savings\n",
            "425 ----> accounts\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "outputId": "71a393b6-9551-426d-8b8c-ca624b23dad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 23]), TensorShape([64, 15]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder and decoder model\n",
        "\n",
        "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://www.tensorflow.org/tutorials/seq2seq). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU\n",
        "\n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "outputId": "31671bbd-4342-43f4-bf9a-ffd107b3bb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 23, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "outputId": "8d5d1c2b-e286-49fa-bc73-6022011e027c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 23, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "04825471-b1df-40d2-bb94-22f4ebcf393c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 760)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "outputId": "7b2e49b7-be6c-4d40-aa35-1fc1dd0bc05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6817
        }
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.3264\n",
            "Epoch 1 Loss 1.3127\n",
            "Time taken for 1 epoch 20.436784982681274 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9322\n",
            "Epoch 2 Loss 1.0173\n",
            "Time taken for 1 epoch 3.0138185024261475 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8330\n",
            "Epoch 3 Loss 0.9506\n",
            "Time taken for 1 epoch 2.519026041030884 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7903\n",
            "Epoch 4 Loss 0.9080\n",
            "Time taken for 1 epoch 2.881924867630005 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7716\n",
            "Epoch 5 Loss 0.8761\n",
            "Time taken for 1 epoch 2.5544722080230713 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.7538\n",
            "Epoch 6 Loss 0.8473\n",
            "Time taken for 1 epoch 3.4340648651123047 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7318\n",
            "Epoch 7 Loss 0.8211\n",
            "Time taken for 1 epoch 2.619783639907837 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7118\n",
            "Epoch 8 Loss 0.7965\n",
            "Time taken for 1 epoch 2.9488720893859863 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6938\n",
            "Epoch 9 Loss 0.7736\n",
            "Time taken for 1 epoch 2.5481255054473877 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6752\n",
            "Epoch 10 Loss 0.7473\n",
            "Time taken for 1 epoch 2.8946220874786377 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6472\n",
            "Epoch 11 Loss 0.7096\n",
            "Time taken for 1 epoch 2.594940423965454 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.6287\n",
            "Epoch 12 Loss 0.6805\n",
            "Time taken for 1 epoch 2.948852300643921 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.6103\n",
            "Epoch 13 Loss 0.6580\n",
            "Time taken for 1 epoch 2.5949840545654297 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.5738\n",
            "Epoch 14 Loss 0.6305\n",
            "Time taken for 1 epoch 2.9356727600097656 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.5548\n",
            "Epoch 15 Loss 0.5967\n",
            "Time taken for 1 epoch 2.6114134788513184 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.5620\n",
            "Epoch 16 Loss 0.5702\n",
            "Time taken for 1 epoch 3.8798675537109375 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5041\n",
            "Epoch 17 Loss 0.5370\n",
            "Time taken for 1 epoch 2.6272270679473877 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4694\n",
            "Epoch 18 Loss 0.4887\n",
            "Time taken for 1 epoch 2.992558479309082 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4395\n",
            "Epoch 19 Loss 0.4412\n",
            "Time taken for 1 epoch 2.623915433883667 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4054\n",
            "Epoch 20 Loss 0.4069\n",
            "Time taken for 1 epoch 3.057619333267212 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.3717\n",
            "Epoch 21 Loss 0.3819\n",
            "Time taken for 1 epoch 2.6337711811065674 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.3404\n",
            "Epoch 22 Loss 0.3640\n",
            "Time taken for 1 epoch 3.208345890045166 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.3269\n",
            "Epoch 23 Loss 0.3489\n",
            "Time taken for 1 epoch 2.676395893096924 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.2898\n",
            "Epoch 24 Loss 0.3235\n",
            "Time taken for 1 epoch 3.1416122913360596 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.2645\n",
            "Epoch 25 Loss 0.2876\n",
            "Time taken for 1 epoch 2.6692392826080322 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.2406\n",
            "Epoch 26 Loss 0.2664\n",
            "Time taken for 1 epoch 3.7580668926239014 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.2054\n",
            "Epoch 27 Loss 0.2372\n",
            "Time taken for 1 epoch 2.6941850185394287 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.1823\n",
            "Epoch 28 Loss 0.2127\n",
            "Time taken for 1 epoch 3.056490659713745 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.1538\n",
            "Epoch 29 Loss 0.1641\n",
            "Time taken for 1 epoch 2.674464464187622 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.1074\n",
            "Epoch 30 Loss 0.1338\n",
            "Time taken for 1 epoch 3.0744194984436035 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0910\n",
            "Epoch 31 Loss 0.1130\n",
            "Time taken for 1 epoch 2.6865153312683105 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0697\n",
            "Epoch 32 Loss 0.1019\n",
            "Time taken for 1 epoch 3.118769407272339 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0481\n",
            "Epoch 33 Loss 0.0877\n",
            "Time taken for 1 epoch 2.7188589572906494 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0567\n",
            "Epoch 34 Loss 0.0782\n",
            "Time taken for 1 epoch 3.1030635833740234 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0492\n",
            "Epoch 35 Loss 0.0700\n",
            "Time taken for 1 epoch 2.6960785388946533 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0302\n",
            "Epoch 36 Loss 0.0639\n",
            "Time taken for 1 epoch 3.1402299404144287 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0356\n",
            "Epoch 37 Loss 0.0546\n",
            "Time taken for 1 epoch 2.6875789165496826 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0316\n",
            "Epoch 38 Loss 0.0458\n",
            "Time taken for 1 epoch 3.078701972961426 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0151\n",
            "Epoch 39 Loss 0.0408\n",
            "Time taken for 1 epoch 2.670743942260742 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0189\n",
            "Epoch 40 Loss 0.0353\n",
            "Time taken for 1 epoch 3.071744441986084 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0198\n",
            "Epoch 41 Loss 0.0335\n",
            "Time taken for 1 epoch 2.671170234680176 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0150\n",
            "Epoch 42 Loss 0.0346\n",
            "Time taken for 1 epoch 3.1054062843322754 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0198\n",
            "Epoch 43 Loss 0.0315\n",
            "Time taken for 1 epoch 2.6501729488372803 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0172\n",
            "Epoch 44 Loss 0.0296\n",
            "Time taken for 1 epoch 3.1496663093566895 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0109\n",
            "Epoch 45 Loss 0.0306\n",
            "Time taken for 1 epoch 2.69903564453125 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0105\n",
            "Epoch 46 Loss 0.0322\n",
            "Time taken for 1 epoch 3.0884854793548584 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0146\n",
            "Epoch 47 Loss 0.0322\n",
            "Time taken for 1 epoch 2.6746444702148438 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0148\n",
            "Epoch 48 Loss 0.0289\n",
            "Time taken for 1 epoch 3.0961642265319824 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0173\n",
            "Epoch 49 Loss 0.0288\n",
            "Time taken for 1 epoch 2.6733171939849854 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0172\n",
            "Epoch 50 Loss 0.0275\n",
            "Time taken for 1 epoch 3.119947671890259 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0127\n",
            "Epoch 51 Loss 0.0308\n",
            "Time taken for 1 epoch 2.6790308952331543 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0089\n",
            "Epoch 52 Loss 0.0310\n",
            "Time taken for 1 epoch 3.0703611373901367 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0076\n",
            "Epoch 53 Loss 0.0252\n",
            "Time taken for 1 epoch 2.661372661590576 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0139\n",
            "Epoch 54 Loss 0.0234\n",
            "Time taken for 1 epoch 3.0437123775482178 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0102\n",
            "Epoch 55 Loss 0.0223\n",
            "Time taken for 1 epoch 2.6656413078308105 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0115\n",
            "Epoch 56 Loss 0.0210\n",
            "Time taken for 1 epoch 3.071354866027832 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0116\n",
            "Epoch 57 Loss 0.0189\n",
            "Time taken for 1 epoch 2.661430597305298 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0131\n",
            "Epoch 58 Loss 0.0160\n",
            "Time taken for 1 epoch 3.0791571140289307 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0082\n",
            "Epoch 59 Loss 0.0134\n",
            "Time taken for 1 epoch 2.686941146850586 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0022\n",
            "Epoch 60 Loss 0.0101\n",
            "Time taken for 1 epoch 4.411203622817993 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0036\n",
            "Epoch 61 Loss 0.0103\n",
            "Time taken for 1 epoch 2.7633538246154785 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0014\n",
            "Epoch 62 Loss 0.0077\n",
            "Time taken for 1 epoch 3.083139181137085 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0016\n",
            "Epoch 63 Loss 0.0078\n",
            "Time taken for 1 epoch 2.689650058746338 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0011\n",
            "Epoch 64 Loss 0.0072\n",
            "Time taken for 1 epoch 4.1665215492248535 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0012\n",
            "Epoch 65 Loss 0.0058\n",
            "Time taken for 1 epoch 2.6942715644836426 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0009\n",
            "Epoch 66 Loss 0.0049\n",
            "Time taken for 1 epoch 3.077727794647217 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0009\n",
            "Epoch 67 Loss 0.0046\n",
            "Time taken for 1 epoch 2.665529489517212 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0008\n",
            "Epoch 68 Loss 0.0041\n",
            "Time taken for 1 epoch 3.189718008041382 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0007\n",
            "Epoch 69 Loss 0.0036\n",
            "Time taken for 1 epoch 2.676623582839966 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0007\n",
            "Epoch 70 Loss 0.0038\n",
            "Time taken for 1 epoch 3.0937933921813965 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0007\n",
            "Epoch 71 Loss 0.0034\n",
            "Time taken for 1 epoch 2.6575510501861572 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0007\n",
            "Epoch 72 Loss 0.0032\n",
            "Time taken for 1 epoch 3.084087371826172 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0006\n",
            "Epoch 73 Loss 0.0027\n",
            "Time taken for 1 epoch 2.652198314666748 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0005\n",
            "Epoch 74 Loss 0.0028\n",
            "Time taken for 1 epoch 3.0657737255096436 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0005\n",
            "Epoch 75 Loss 0.0027\n",
            "Time taken for 1 epoch 2.6779887676239014 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0008\n",
            "Epoch 76 Loss 0.0039\n",
            "Time taken for 1 epoch 3.155811309814453 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0007\n",
            "Epoch 77 Loss 0.0047\n",
            "Time taken for 1 epoch 2.6561317443847656 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0009\n",
            "Epoch 78 Loss 0.0050\n",
            "Time taken for 1 epoch 3.21461820602417 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0008\n",
            "Epoch 79 Loss 0.0034\n",
            "Time taken for 1 epoch 2.671245813369751 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0006\n",
            "Epoch 80 Loss 0.0038\n",
            "Time taken for 1 epoch 3.0491890907287598 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0007\n",
            "Epoch 81 Loss 0.0058\n",
            "Time taken for 1 epoch 2.650644063949585 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0012\n",
            "Epoch 82 Loss 0.0067\n",
            "Time taken for 1 epoch 3.138676881790161 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0009\n",
            "Epoch 83 Loss 0.0064\n",
            "Time taken for 1 epoch 2.6564652919769287 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0015\n",
            "Epoch 84 Loss 0.0069\n",
            "Time taken for 1 epoch 3.021768093109131 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0016\n",
            "Epoch 85 Loss 0.0075\n",
            "Time taken for 1 epoch 2.6542110443115234 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0007\n",
            "Epoch 86 Loss 0.0088\n",
            "Time taken for 1 epoch 3.1514875888824463 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0010\n",
            "Epoch 87 Loss 0.0115\n",
            "Time taken for 1 epoch 2.655411958694458 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0052\n",
            "Epoch 88 Loss 0.0128\n",
            "Time taken for 1 epoch 3.0347142219543457 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0078\n",
            "Epoch 89 Loss 0.0141\n",
            "Time taken for 1 epoch 2.691873788833618 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0065\n",
            "Epoch 90 Loss 0.0165\n",
            "Time taken for 1 epoch 3.0507168769836426 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0052\n",
            "Epoch 91 Loss 0.0238\n",
            "Time taken for 1 epoch 2.664639711380005 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0123\n",
            "Epoch 92 Loss 0.0223\n",
            "Time taken for 1 epoch 3.0382261276245117 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0143\n",
            "Epoch 93 Loss 0.0212\n",
            "Time taken for 1 epoch 2.6649129390716553 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0086\n",
            "Epoch 94 Loss 0.0188\n",
            "Time taken for 1 epoch 3.0904459953308105 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0045\n",
            "Epoch 95 Loss 0.0211\n",
            "Time taken for 1 epoch 2.6578028202056885 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0137\n",
            "Epoch 96 Loss 0.0199\n",
            "Time taken for 1 epoch 3.419660806655884 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0142\n",
            "Epoch 97 Loss 0.0179\n",
            "Time taken for 1 epoch 2.6765246391296387 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0134\n",
            "Epoch 98 Loss 0.0165\n",
            "Time taken for 1 epoch 3.058131694793701 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0163\n",
            "Epoch 99 Loss 0.0201\n",
            "Time taken for 1 epoch 2.661160469055176 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0177\n",
            "Epoch 100 Loss 0.0195\n",
            "Time taken for 1 epoch 3.0816614627838135 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "outputId": "49efda67-3270-4fc0-c76a-75e4822213b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb3a0320b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "outputId": "66fb37a0-3c24-4007-8e78-afee4b72452d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "translate(u'best paper notebook')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> best paper notebook <end>\n",
            "Predicted translation: paper <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAGpCAYAAAAeHUstAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8AAABiPgAQAM5v8DV5sv9XP7jRUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSx2iM36EZQZ",
        "outputId": "4b8bcea9-4c3c-420e-8ac2-ef0a8b790f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "translate(u'best notebook ever')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> best notebook ever <end>\n",
            "Predicted translation: bittorrent <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAGpCAYAAADhk6yKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHdNJREFUeJzt3Xu4bed8L/DvLzuSSDRS91BxCeoW\nl0iFQ1yaulZ7VB2n4hK0oihVVa3TuhwtLVIVh3OItkiD1nH0QVXUJe6XEFQjKlISTSN1FxFJSH7n\njzE3a6937Vsua6y11+fzPPNZc75jzDl/a49nr/md7xjv+1Z3BwAAltpt7gIAAFh7hEQAAAZCIgAA\nAyERAICBkAgAwEBIBABgICQCADAQEgEAGAiJAAAMhESANaKqrryNbT+zmrUACIkAa8ebq2r35Y1V\ndf0kJ85QD7CBCYkAa8e+SY5b2rAIiO9L8rE5CgI2LiERYO14QJJbV9XLk6SqDsgUED+S5JEz1gVs\nQNXdc9cAwEJVXTfJh5K8Pcn9k3w4yZHtjzWwyoREgDWmqg7MFBTf2d2PmrkcYIMSEgFmVFXfS7LS\nH+I9k/wwySWbG7p739WqC2AYRQfAqvqtuQsAWImeRAAABnoSAdaQqtozycOS3DLTaejPJXlDd184\na2HAhqMnEWCNqKpbJjkh03yJ/7JoPijJd5Pct7s/P1dtwMZjnkS2qqpuWlXvraqD5q4FNohjknw6\nyQHdfVh3H5bkgCT/nOQls1YGbDhCIttyZJJ7JHnMzHXARnGXJP+ju8/d3LC4/4dJ7jpbVcCGJCSy\noqqqJI9I8tdJjqiqTTOXBBvBBUn2W6H9qottAKtGSGRr7pHkp5I8OcmPMq38AFyx3pbkVVV1l6ra\ntLjdNckrk7x15tqADUZIZGuOTPKm7j4/yd8uHgNXrN9O8sUkH8zUc3hBkvcnOS3JU2asC9iAjG5m\nUFX7JPlqkl/s7g9W1e2SfDTJ/t39nXmrg11fVd00yc0XDz/f3afPWQ+wMelJZCW/muQb3f3BJOnu\nz2Tq3fi1WauCDaK7v5jkxCTvFRBhdVXVPlX1yKq66ty1zE1IZCWPSHL8srbjkzxq9UuBjaWqnlhV\nX8k0N+K5VXVmVT1h7rpgA3lIkldn+izc0JxuZgtVdf0kX05yi0Vvxub2n0lyRpJbdvdpM5UHu7Sq\n+h9JnpHk6CQfWjQfluSpSZ7f3X82V22wUVTViUmuneT87j5k7nrmJCQCrBGLHsTf7+43LGt/WKaQ\neIN5KoONoapumGmg2B2TfCzJwd196pw1zcnpZgZVdcBinsQVt612PbCBXCvJJ1ZoPylTzwZwxXpE\nkg8ursX/x2zwmT2ERFby5STXXN5YVVdfbAOuGKclOWKF9iOSfGGVa4GN6JFJ/mZx/3VJHra1TpON\nYPe5C2BNqiQrXYdwlVj1Aa5Iz0nyxqq6W5IPL9rukuTuSf7bXEXBRlBV/yXJ/knetGh6W5JXJfmF\nJO+aq645CYn8WFW9dHG3k/xpVZ2/ZPOmTNdofGbVC4MNorvfXFWHJvmdJA9YNH8+yR27+9PzVQYb\nwpFJ3tLd5yVJd19UVW/MNLPHhgyJBq7wY4sRXcnUa/HRJBct2XxRptHNRy8d9QwA611V7ZnknCQP\n7e4TlrTfNck7k1x7c3jcSIREtrC49uKNSR7T3d+bux7YaKpqr0zXIN5y0XRqkjd09w/mqwp2bVV1\njST3T3J8d1+ybNvDk7y7u8+ZpbgZCYlsoao2Zbru8LYbedg/zKGqDs50HdTeSf5l0XzrJBdmWibz\nU3PVBmw8Rjezhe6+OMmZSfaYuxbYgI7NNGDlZ7r7bt19tyTXT/KBxTaAVaMnkUFVHZnkoUke3t3f\nmLse2Ciq6gdJ7rC8F7+qbpXkk9195Xkqg11TVX05K8/mMejuG1/B5aw5RjezkqcluVGS/6iqs5J8\nf+nG7r7NLFXBru9fk1w303WIS+2faQ5F4PL1siX3r5JpCcyTMg3eTJI7Z5rZ489Xua41QUhkJW/a\n/i7A5aGqrrbk4R8leWlVPTfTkmBJcqdF+x+sdm2wq+vuH4e/qnpNkhd09/OX7lNVz0hyq1UubU1w\nuhlgRlV1SbY83bV5dYde/ri7N61aYbDBVNW5mdZqPn1Z+02SfKq7952nsvnoSQSY1z3nLoDLpqp2\nT3LvJB/v7m/OXQ+X2veT3CPJ6cva75Hk/OU7bwRCIoOq2iPJH2YavHJAkist3a43Ay4/3f3+uWvg\nsunuH1XVm5PcPImQuH79RZKXV9Uh2fJyjyMzLZm54ZgCh5X8cab/FH+e5JIkv5fk5Zn++D1hxrpg\nl1dVB1XVy6rqHVW1/6LtgVV1+7lrY5v+OclN5i6CS6+7X5jkEUkOSvLixe2gJEd29wvmrG0urklk\nsJgS4PHdfUJVfS/J7br736rq8UkO7+4Hz1wi21BVFyfZv7u/tqz96km+pid47aqqeyd5a5J3ZFr9\n4Rbd/aWq+t0kh3X3A2ctkK2qqvsl+bMkz05ycsZZIb41R11wWTjdzEqunZ9MwXFekv0W909IsiG/\nTa0ztZX2PbPletysPX+c5Knd/b8XX9A2e1+S352nJHbQ2xc/35xxIFIn8eVsHamq/bLsbOtGDPpC\nIiv5Sqa52r6S6QLe+2T6ZnznJNaPXaOq6qmLu53kN6tq6WL0m5IclmkePtauWyf5xxXav5Xkaiu0\ns3YYgLTOVdUNkrwi00CVpauObdigLySykr9PcnimC3ePSfKGqnpskusledGchbFNT1r8rCS/keTi\nJdsuSnJGkt9c5ZrYOd/K9P/sjGXtByc5a9WrYYcZgLRLeHWmM2e/nuTs7OBKLLsy1ySyXVV1aJK7\nJDmtu/9h7nrYtqo6McmDuvvbc9fCzqmqF2Tq8X1Ipks+Dsm02sprkry6u587X3VsT1UdlORxSQ5M\n8pju/mpVPTDJmd396XmrY3sWZ1/u1N2nzF3LWmF0M4Oqutti3q8kSXd/vLtfnOSEqrrbjKWxA7r7\nnssDYlXdpKr2mqsmdtgfJflykjMzLRF2apL3JvlQkufNWBfbsRh09IlMPcE/n2TzOtsHZhrMwtr3\n5UzXbrOgJ5GB0bHrW1U9P8kXuvu1VVVJ3pXpQ+u7Se7b3R+ftUC2q6punOkU825JPt3dX5y5JLaj\nqj6e5LVLBh3ddjEy/Q5J3tbd1525RLajqn4+0/KXT1i+6spGpSeRlWy+SHe5q2fZtA6sSQ9L8oXF\n/fsluW2mCWGPyzRFB2tUVT2rqvbu7i9195u6+43d/cWqunJVPWvu+tgmg47Wv7dkGrTyhao6v6rO\nXXqbubZZGLjCj1XVWxd3O8nxVXXhks2bMv0R/MiqF8bOunZ+Msjh/kne2N0nVdW3knxyvrLYAc/O\nNLpy+RJgey+2uSZx7TLoaP37rbkLWGuERJbavJxUJfl2tpzu5qJM10W9arWLYqd9M8kNMn0w3TvT\n6ZNk+v++tTkUWRu21ot/+0whhLXr9UleVFUPyXQMd6+quyc5OtOoWda47n7t3DWsNUIiP9bdj06S\nqjojydHd7dTy+vT/kry+qk7LdJrrnYv222VcuJ41YHENWy9uX6qqpUFxU5K9MvUwsnb9UaZR6Gdm\nCvunLn6+PgYdrRtVde1MS/MdmOSZ3f2NqrpLkrO7+8vzVrf6DFxhUFW7JUl3X7J4fJ0kD0hyanc7\n3bzGLUam/3aSA5K8ZvPUG1X1O0m+191/OWd9jKrqyEyB4q+TPCXTIKPNLkpyRnd/dI7a2DlVdWCm\nnl+DjtaZxSCj92Qa5XyrJDdfDD56TpKbdfcRc9Y3ByGRQVW9I8kJ3X1MVV0l0yod+2SakuPXu/u4\nWQuEXdTi9ORHuvuHc9fCzlnMh/h2x279Wswx+4HufvayEep3TvK33X2DmUtcdUY3s5JDMs3NliQP\nSnJukmsleWySp81VFDuuqg6qqpdV1Tuqav9F2wOr6vZz18bWLVbt2K2qHlNVR1fVi6rqUVVl7ra1\n7/VJzqmqVyxOT7L+3CHJStclfjXTgMANR0hkJVdJ8p3F/Xsn+fvFt+P3ZrpOgzXMpL7rV1XdMskX\nk7w4yaGZpi56SZLTquoWc9bGdl0705foA5O8v6q+VFV/UlU3n7kudtwPkvz0Cu03T/K1Fdp3eUIi\nK/lKkrtU1T5J7pNpMuZkGgSxfGoO1p4/TvLU7v6VTNezbfa+JHecpSJ21DFJPp3kgO4+rLsPy3Rt\n6T9nCousUd39ve5+dXffK9Mxe1mS+yb5XFV9Yt7q2EFvSfLsJT33XVU3TPKCTAMCNxzXJDKoqsdl\n+gN3XqaRegd39yVV9eQkD+zun5+1QLapqr6f5Fbdfcay62pulOTz3W15vjWqqs5P8nPd/bll7Qcl\n+Vh37zNPZeysqtojyS9lGvV8GytVrX1VtW+mCdFvk+k6/HMy9RB/JMn9NuKMH6bAYdDdr6yqT2b6\nNvyuzaOck/xbkmfOVxk7yKS+69cFSfZbof2qi22scVV1z0yrHv3qounNSZ46X0XsqO4+N8ldF8vz\nbV4W81Pd/e55K5uPkMgWquqqmb71fjDJycs2fyfT3F+sbSb1Xb/eluRVVfXYJB9btN05ySuTvHWr\nz2J2VfWiJL+WaZDfCUmOSvLW7r5wm09kTVj62dfd781PBm9mMRDp1O7+9mwFzsTpZrZQVT+VaSTX\nfbr7w0vab5vkpCTX6+5vzFUf21dVV8o0qe+vZZp775JM34hfl+RR3X3xfNWxLVW1X6bRlb+UZPNx\n2pTpWqlHd/d3tvZc5lVVH05yfJK/626r46wzPvtWJiQyqKrXJTmvux+3pO3oTJOJ/vJ8lbEzqurG\nSe6aqTfxo91ttZV1oqpukmTzaObPO3brQ1XdL8kTk9w4U9j496r6jSRf7u73zFsd2+Ozb+R0Mys5\nLskbqupJ3X3RYgWWI2Lx83Wjqp6S6Tqo6y2azq6qFyd5SftmuKZV1X9Pcnim05a7LdqSJBv1g2o9\nqKqHZVo68S8zHb8rLTZtSvL0TCt5sLb57FvGFDis5F2Z5ot6wOLx4Un2yHS9FGtcVb0wyXMyXcd2\nr8XtFUmelWkqB9aoxXVtxye5YaZrgL+57Mba9fQkj+3u30nyoyXtH8u0bjprn8++ZZxuZkVV9YIk\nP9vdD6yq4zKt+fvEueti+6rqW0mO6u43LWt/cJJXdvfV56mM7amq/0zyxOXHjrVvMX3RLbr7zGVT\nTx2Y5JTuvvJ2XoI1wGfflpxuZmuOS3JyVR2Q5FcyfaNi/fjsVtqcPVjbdkvymbmL4FI5O8nNMs0t\nu9TdMk0fxvrgs28JHxisaDGZ7ymZRsSe1d0nzVwSO+64TBfPL/f4JH+zyrWwc45N8vC5i+BSOTbJ\nS5es23z9qjoyyQuT/J/5ymJn+Ozbkp5EtuW4TEuB/eHchbBtVfXSJQ93T/LwqrpPfjLX3qFJrpvp\nDx9r135Jjqiqe2Xq+f3h0o3d/eRZqmK7uvuFi7n23pVkryQnJrkwydHd/fJZi2Nn+exbcE0iW1VV\nV0vypEzXsZ0zdz1sXVWduIO7tmUV167tHEfHbh2oqr2T3DLTmbpTu/u8mUtiJ/ns+wkhEQCAgWsS\nAQAYCIlsV1UdNXcNXDqO3frm+K1vjt/65dhNhER2hP8s65djt745fuub47d+OXYREgEAWIGBK5eD\nPWrP3iv7zF3GFeaHuTBXyp5zl8Gl4Nitb47f+rYrH7+b3eb8uUu4Qn39mxfnmlffNHcZV5iTP3vh\nN7r7mtvbzzyJl4O9sk8OrQ09KTsAG8g732lhoPVs0/6nL18ZaEVONwMAMBASAQAYCIkAAAyERAAA\nBkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgE\nAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyE\nRAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADA\nQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMNhuSKyq91XV\nyy7tdgAA1p/LoyfxQUmesflBVZ1RVU9bukNVPaqqzrsc3usKt55qBQC4oux+WV+gu791eRSyI6pq\ntyTV3Rcva9+juy9arToAAHZ1O9qTuHtVHVNV317cXrQIbFucbq6q9yW5QZIXVVUvbvdI8uok+yxp\ne85i/5+uqtcuXvMHVfXuqrrV5jfd3KtXVfevqlOSXJTkFlX1mqr6h6r6/ao6K8lZi/33qKoXVNVZ\nVXV+VX2iqu6z5PXusXj/w6vq44t9PllVB2/evrVaAQA2kh0NiQ9b7HvnJI9LclSSp6yw34MyBbbn\nJtl/cfvIYt/zl7Qdvdj/NUkOTfJfk9xxsc8JVXXlJa+5V5JnLt73lknOXLTfPcltktw3yeGLtlcv\n2o9Icuskr03ytqq67bI6/zTJHyQ5OMk3k7yuqmo7tQIAbBg7err5q0me3N2d5F+r6mZJnprkxUt3\n6u5vVdXFSb7X3edsbq+q706bt2i7aZJfTnL37v7Aou0RSb6SKZT+5WLXTUl+q7tPXvLcJLkgyWO6\n+8JF24FJHprkht39lcWuL6uqX8gUMJ+wpNRndveJi+c9N8mHklyvu89aqdaVVNVRmcJy9sre29oV\nAGDd2dGexI8tAuJmH01yvara9zK89y2SXLJ4rSRJd383yb9k6jHc7EdJPrPC80/ZHBAXDk5SSU5d\nnKI+bzEA5ReTHLjsuZ9dcv/sxc9r7Uzx3X1sdx/S3YdcKXvuzFMBANa8yzxw5QqyNJBeuHygysL3\nlz3ebfG8n0vyw2XbfrDs8dLtm9/LnJEAAAs7GowOXVyzt9mdkpzd3eeusO9FmU4Rb6/t8/nJdY5J\nkkXP5EFJTt3Bupb6dKaexOt09+nLbv+xE6+zUq0AABvKjobE6yZ5SVX9bFU9OMnvJfmLrex7RpLD\nqup6VXWNJW17VdW9quoaVbV3d38xyVuSvLKqDquqg5Icn+TcJK/f2V+ku09L8rokr6mqB1fVjavq\nkKp6WlU9aCdeaqh1Z2sBAFjvdjQkvi5T79rHk7wqyV9l6yHxWUmun+Tfknw9Sbr7I0lekeQNi7an\nL/Z9dJKTkrx18XPvJPft7uWnh3fUozONcH5hkn9N8g9J7pafjIjerm3UCgCwYdSW41G4NPatq/Wh\ndfj2dwSAXcA7z15pPCnrxab9Tz+5uw/Z3n4GawAAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQ\nCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAY\nCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREA\ngIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBAS\nAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGCw+9wFAADry/1ufKe5S+AyOX2H9tKTCADA\nQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkA\nAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQ\nCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAY\nCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREA\ngIGQCADAQEgEAGAgJAIAMNhQIbGqnlZVZ8xdBwDAWrehQiIAADtmzYTEqtq3qvZb5fe8ZlXttZrv\nCQCwHswaEqtqU1Xdp6pen+ScJLddtF+1qo6tqq9V1feq6v1VdciS5z2qqs6rqsOr6pSq+n5VnVhV\nN1r2+k+vqnMW+x6X5CrLSrh/knMW73WXK/jXBQBYN2YJiVV1q6p6YZJ/T/J3Sb6f5L5JPlBVleTt\nSa6X5AFJbp/kA0neW1X7L3mZPZM8I8ljktw5yX5JXrHkPR6S5E+SPDvJwUm+kOSpy0p5XZIjkvxU\nkndV1elV9azlYRMAYKNZtZBYVVevqidX1clJPp3k5kl+O8l1uvux3f2B7u4k90xyuyQP7u6Tuvv0\n7n5mki8lecSSl9w9yRMX+3w2ydFJ7rEImUnylCSv7e5Xdvdp3f28JCctram7f9Td/9jdD01ynSTP\nX7z/F6vqfVX1mKpa3vu4+fc5qqo+WVWf/GEuvHz+kQAA1ojV7El8UpJjklyQ5Gbd/cvd/X+7+4Jl\n+90hyd5Jvr44TXxeVZ2X5NZJDlyy34Xd/YUlj89OskeSn148vkWSjy577eWPf6y7z+3uv+7ueyb5\nuSTXTvJXSR68lf2P7e5DuvuQK2XPbfzaAADrz+6r+F7HJvlhkkcmOaWq/j7J3yR5T3dfvGS/3ZL8\nZ5LDVniNc5fc/9Gybb3k+TutqvbMdHr74ZmuVfxcpt7It1ya1wMAWM9WrSexu8/u7ud1988m+YUk\n5yX52yRnVdWfV9XtFrt+KlMv3iWLU81Lb1/bibf8fJI7LWvb4nFN7lpVr8w0cOZ/JTk9yR26++Du\nPqa7v73zvy0AwPo2y8CV7v5Ydz8+yf6ZTkPfLMknquqwJO9O8uEkb6mq+1XVjarqzlX1Pxfbd9Qx\nSY6sqsdW1U2r6hlJDl22z8OT/FOSfZM8NMn1u/v3uvuUy/grAgCsa6t5unnQ3RcmeVOSN1XVtZJc\n3N1dVffPNDL5VUmulen084eTHLcTr/13VXXjJM/LdI3jW5O8OMmjluz2nkwDZ84dXwEAYOOqaUAx\nl8W+dbU+tA6fuwwAWBW77WUdivXsn35w/Mndfcj29lszK64AALB2CIkAAAyERAAABkIiAAADIREA\ngIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBAS\nAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAAD\nIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIA\nMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMNh97gIAgPXlkgsumLsEVoGe\nRAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADA\nQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkA\nAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQ\nCADAQEgEAGAgJAIAMBASAQAYCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAY\nCIkAAAyERAAABkIiAAADIREAgIGQCADAQEgEAGAgJAIAMBASAQAYCIkAAAx2n7uA9aqqjkpyVJLs\nlb1nrgYA4PKlJ/FS6u5ju/uQ7j7kStlz7nIAAC5XQiIAAAMhEQCAgZAIAMBASAQAYCAkAgAwEBIB\nABgIiQAADIREAAAGQiIAAAMhEQCAgZAIAMBASAQAYCAkAgAwEBIBABgIiQAADIREAAAGQiIAAAMh\nEQCAgZAIAMBASAQAYCAkAgAwEBIBABgIiQAADIREAAAGQiIAAAMhEQCAgZAIAMBASAQAYCAkAgAw\nEBIBABgIiQAADIREAAAGQiIAAAMhEQCAgZAIAMBASAQAYCAkAgAwEBIBABgIiQAADIREAAAGQiIA\nAAMhEQCAgZAIAMBASAQAYCAkAgAwEBIBABgIiQAADIREAAAGQiIAAAMhEQCAgZAIAMBASAQAYCAk\nAgAwqO6eu4Z1r6q+nuTMueu4Al0jyTfmLoJLxbFb3xy/9c3xW7929WN3g+6+5vZ2EhLZrqr6ZHcf\nMncd7DzHbn1z/NY3x2/9cuwmTjcDADAQEgEAGAiJ7Ihj5y6AS82xW98cv/XN8Vu/HLu4JhEAgBXo\nSQQAYCAkAgAwEBIBABgIiQAADIREAAAG/x8LamBa2emDKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3LLCx3ZE0Ls",
        "outputId": "8c1d3987-fbeb-4ac4-a9ae-5ed6ec76d56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "translate(u'five best sites for finding deals online')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> five best sites for finding deals online <end>\n",
            "Predicted translation: deals <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAELCAYAAABKyfvIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DUQVLVqUE1YW",
        "outputId": "252c9f41-4e91-41a0-bcf5-14a2f633b1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "# wrong translation\n",
        "translate(u'make your own firefox site search plug-in')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> make your own firefox site search plug in <end>\n",
            "Predicted translation: firefox search <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAEwCAYAAAAkdAymAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn6JaMzwYOwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMO3g6g4N7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68419868-d9da-4eb2-c4da-58e5368001d0"
      },
      "source": [
        "  from tensorflow.python.util import compat\n",
        "  #where to save to?\n",
        "  export_path_base = 'model'\n",
        "  export_path = os.path.join(\n",
        "      compat.as_bytes(export_path_base))\n",
        "  print ('Exporting trained model to', export_path)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exporting trained model to b'model'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPUn8Eyc0ffQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "176756c9-3639-42d5-fe1b-e54069658c45"
      },
      "source": [
        "from tensorflow.python.saved_model import builder as saved_model_builder\n",
        "from tensorflow.python.saved_model import signature_constants\n",
        "from tensorflow.python.saved_model import signature_def_utils\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.python.saved_model import utils\n",
        "  \n",
        "  #This creates a SERVABLE from our model\n",
        "  #saves a \"snapshot\" of the trained model to reliable storage \n",
        "  #so that it can be loaded later for inference.\n",
        "  #can save as many version as necessary\n",
        "  \n",
        "  #the tensoroflow serving main file tensorflow_model_server\n",
        "  #will create a SOURCE out of it, the source\n",
        "  #can house state that is shared across multiple servables \n",
        "  #or versions\n",
        "  \n",
        "  #we can later create a LOADER from it using tf.saved_model.loader.load\n",
        "  \n",
        "  #then the MANAGER decides how to handle its lifecycle\n",
        "  \n",
        "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
        "\n",
        "  # Build the signature_def_map.\n",
        "  #Signature specifies what type of model is being exported, \n",
        "  #and the input/output tensors to bind to when running inference.\n",
        "  #think of them as annotiations on the graph for serving\n",
        "  #we can use them a number of ways\n",
        "  #grabbing whatever inputs/outputs/models we want either on server\n",
        "  #or via client\n",
        "classification_inputs = utils.build_tensor_info(serialized_tf_example)\n",
        "classification_outputs_classes = utils.build_tensor_info(prediction_classes)\n",
        "classification_outputs_scores = utils.build_tensor_info(values)\n",
        "\n",
        "   \n",
        "classification_signature = signature_def_utils.build_signature_def(\n",
        "    inputs={signature_constants.CLASSIFY_INPUTS: classification_inputs},\n",
        "    outputs={\n",
        "        signature_constants.CLASSIFY_OUTPUT_CLASSES:\n",
        "            classification_outputs_classes,\n",
        "        signature_constants.CLASSIFY_OUTPUT_SCORES:\n",
        "            classification_outputs_scores\n",
        "    },\n",
        "    method_name=signature_constants.CLASSIFY_METHOD_NAME)\n",
        "\n",
        "tensor_info_x = utils.build_tensor_info(x)\n",
        "tensor_info_y = utils.build_tensor_info(y)\n",
        "\n",
        "prediction_signature = signature_def_utils.build_signature_def(\n",
        "    inputs={'images': tensor_info_x},\n",
        "    outputs={'scores': tensor_info_y},\n",
        "    method_name=signature_constants.PREDICT_METHOD_NAME)\n",
        "legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
        "  \n",
        "  #add the sigs to the servable\n",
        "builder.add_meta_graph_and_variables(\n",
        "    sess, [tag_constants.SERVING],\n",
        "    signature_def_map={\n",
        "        'predict_images':\n",
        "            prediction_signature,\n",
        "        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
        "            classification_signature,\n",
        "    },\n",
        "    legacy_init_op=legacy_init_op)\n",
        " #save it!\n",
        "builder.save()\n",
        "\n",
        "print ('Done exporting!')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ea37a03b56aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#grabbing whatever inputs/outputs/models we want either on server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m#or via client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mclassification_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_tf_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mclassification_outputs_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mclassification_outputs_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'serialized_tf_example' is not defined"
          ]
        }
      ]
    }
  ]
}